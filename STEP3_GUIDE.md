# Step 3: Forward Audio to OpenAI Realtime API

## Overview

In Step 3, we integrate OpenAI's Realtime API to:
- Receive audio from LiveKit
- Convert audio format (48kHz PCM â†’ 24kHz PCM16 base64)
- Send audio to OpenAI via WebSocket
- Receive transcriptions and AI responses

## What We Built

### **Audio Format Conversion**

LiveKit provides audio at **48kHz PCM**, but OpenAI Realtime API expects **24kHz PCM16 base64**.

**`AudioConverter` class:**
- Downsamples from 48kHz to 24kHz (takes every other sample)
- Converts to base64 encoding
- Maintains Int16 format

```javascript
const base64Audio = AudioConverter.convertForOpenAI(frame);
// Input: 48kHz PCM frame from LiveKit
// Output: 24kHz PCM16 base64 string for OpenAI
```

---

### **OpenAI Realtime Session**

**`OpenAIRealtimeSession` class:**
- WebSocket connection to `wss://api.openai.com/v1/realtime`
- Session initialization with voice settings
- Audio streaming with `input_audio_buffer.append`
- Event handling for transcriptions and responses

**Key Features:**
- âœ… Server-side VAD (Voice Activity Detection)
- âœ… Automatic transcription (Whisper)
- âœ… Text and audio responses
- âœ… Streaming audio output

---

### **Integration Flow**

```
User speaks
    â†“
LiveKit (48kHz PCM)
    â†“
AudioConverter (downsample to 24kHz)
    â†“
Base64 encoding
    â†“
OpenAI Realtime API
    â†“
Transcription + AI Response
```

---

## Configuration

### **1. Get OpenAI API Key**

Visit: https://platform.openai.com/api-keys

Create a new API key with access to GPT-4 Realtime models.

### **2. Update `.env` File**

Edit `backend/.env`:

```env
OPENAI_API_KEY=sk-proj-your-actual-key-here
```

âš ï¸ **Important:** Replace `your_openai_api_key_here` with your actual API key!

---

## How It Works

### **Connection Sequence**

1. **AI agent starts** â†’ Connects to OpenAI first
2. **OpenAI session created** â†’ Receives session ID
3. **Session configured** â†’ Sets voice, format, VAD settings
4. **Connects to LiveKit** â†’ Joins room as participant
5. **Subscribes to user audio** â†’ Starts receiving frames
6. **Forwards to OpenAI** â†’ Streams audio continuously

---

### **Audio Streaming**

Every audio frame from LiveKit:
```javascript
// 1. Receive from LiveKit (48kHz)
for await (const frame of audioStream) {
  
  // 2. Convert format (24kHz base64)
  const base64Audio = AudioConverter.convertForOpenAI(frame);
  
  // 3. Send to OpenAI
  this.openai.sendAudio(base64Audio);
}
```

OpenAI automatically:
- Detects speech (VAD)
- Transcribes audio (Whisper)
- Generates response (GPT-4)
- Sends back audio + text

---

### **OpenAI Events**

**Session events:**
- `session.created` - Connection successful
- `session.updated` - Configuration applied

**Audio input events:**
- `input_audio_buffer.speech_started` - User started speaking
- `input_audio_buffer.speech_stopped` - User stopped speaking
- `input_audio_buffer.committed` - Audio buffer processed

**Transcription events:**
- `conversation.item.input_audio_transcription.completed` - User's speech transcribed

**Response events:**
- `response.audio_transcript.delta` - AI response text (streaming)
- `response.audio_transcript.done` - Full AI response text
- `response.audio.delta` - AI response audio (Step 4)
- `response.done` - Response complete

---

## Testing Step 3

### **Prerequisites**

1. All servers running (LiveKit, backend, frontend)
2. OpenAI API key configured in `.env`
3. User connected to room with microphone enabled

---

### **Test 3.1 - OpenAI Connection Test** âœ…

**Goal:** Verify AI agent can connect to OpenAI Realtime API

**How to run:**
```bash
cd backend
npm run agent
```

**Expected output:**
```
[STEP 3] Connecting to OpenAI Realtime API...
[OPENAI] Connecting to Realtime API...
[OPENAI] URL: wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17
[OPENAI] âœ… WebSocket connected
[OPENAI] Initializing session...
[OPENAI] Session configuration sent
[OPENAI] âœ… TEST 3.1 PASS: Session created
[OPENAI] Session ID: sess_xxxxxxxxxxxxx
[OPENAI] Model: gpt-4o-realtime-preview-2024-12-17
[STEP 3] âœ… OpenAI connected
```

**Pass criteria:**
- âœ… WebSocket connects successfully
- âœ… Session created event received
- âœ… Session ID displayed
- âœ… No auth errors

**Fail criteria:**
- âŒ `401 Unauthorized` - Invalid API key
- âŒ Connection timeout
- âŒ WebSocket error

---

### **Test 3.2 - Audio Send Test** âœ…

**Goal:** Verify audio chunks are sent to OpenAI

**How to test:**
1. AI agent connected (Test 3.1 passed)
2. User connected in browser
3. User speaks into microphone

**Expected output:**
```
[OPENAI] âœ… TEST 3.2: Sent audio chunk #1 (0.23 KB)
[OPENAI] âœ… TEST 3.2: Sent audio chunk #2 (0.23 KB)
[OPENAI] âœ… TEST 3.2: Sent audio chunk #3 (0.23 KB)
[OPENAI] ğŸ¤ Speech detected (VAD)
```

**Pass criteria:**
- âœ… Audio chunks sent while user speaks
- âœ… Chunk sizes are non-zero (~0.2-0.3 KB each)
- âœ… VAD detects speech start
- âœ… Continuous streaming (not just one chunk)

**Fail criteria:**
- âŒ No chunks sent
- âŒ Zero-length buffers
- âŒ Chunks sent but no VAD detection

---

### **Test 3.3 - STT Sanity Test (Text Response)** âœ…

**Goal:** Verify OpenAI transcribes user speech correctly

**How to test:**
1. Tests 3.1 and 3.2 passed
2. User speaks clearly: **"Hello, can you hear me?"**
3. Wait 1-2 seconds after speaking
4. Check AI agent logs

**Expected output:**
```
[OPENAI] ğŸ¤ Speech detected (VAD)
[OPENAI] ğŸ”‡ Speech ended (VAD)
[OPENAI] Audio buffer committed
[OPENAI] ğŸ“ Transcription: "Hello, can you hear me?"
[OPENAI] âœ… TEST 3.3 PASS: Received text transcription
[OPENAI] ğŸ¤– AI Response: "Yes, I can hear you! How can I help you today?"
[OPENAI] Response completed
```

**Pass criteria:**
- âœ… Transcription matches what user said
- âœ… Text is accurate (not garbage)
- âœ… AI response is relevant
- âœ… Response arrives within 2-3 seconds

**Fail criteria:**
- âŒ Transcription is gibberish
- âŒ No transcription received
- âŒ Response delayed forever (>10 seconds)
- âŒ Empty or error responses

---

## Architecture Now

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚         â”‚   LiveKit    â”‚         â”‚  AI Agent   â”‚
â”‚   (user-1)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Server     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ (backend)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                        â”‚                        â”‚
      â”‚ Speak (48kHz)         â”‚                        â”‚
      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                        â”‚
      â”‚                        â”‚ Stream audio           â”‚
      â”‚                        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
      â”‚                        â”‚                        â”‚
      â”‚                        â”‚                        â†“
      â”‚                        â”‚                  Convert to 24kHz
      â”‚                        â”‚                        â”‚
      â”‚                        â”‚                        â†“
      â”‚                        â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                        â”‚                 â”‚   OpenAI     â”‚
      â”‚                        â”‚                 â”‚  Realtime    â”‚
      â”‚                        â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                        â”‚                        â”‚
      â”‚                        â”‚                 Transcribe + AI
      â”‚                        â”‚                        â”‚
      â”‚                        â”‚                        â†“
      â”‚                        â”‚                 ğŸ“ "Hello..."
      â”‚                        â”‚                 ğŸ¤– "Yes, I can..."
```

---

## OpenAI Session Configuration

```javascript
{
  modalities: ['text', 'audio'],
  instructions: 'You are a helpful voice assistant. Keep responses concise and natural.',
  voice: 'alloy',
  input_audio_format: 'pcm16',
  output_audio_format: 'pcm16',
  input_audio_transcription: {
    model: 'whisper-1'
  },
  turn_detection: {
    type: 'server_vad',
    threshold: 0.5,
    prefix_padding_ms: 300,
    silence_duration_ms: 500
  }
}
```

**Key settings:**
- **Voice:** `alloy` (can change to: echo, fable, onyx, nova, shimmer)
- **VAD threshold:** 0.5 (sensitivity to speech detection)
- **Silence duration:** 500ms (how long to wait before ending turn)

---

## Troubleshooting

### **Error: 401 Unauthorized**
```
[OPENAI] âŒ WebSocket error: Unexpected server response: 401
```

**Fix:**
1. Check API key in `backend/.env`
2. Verify key starts with `sk-proj-` or `sk-`
3. Ensure key has Realtime API access
4. Check key hasn't expired

---

### **Error: Connection timeout**
```
[OPENAI] âŒ Connection failed: OpenAI connection timeout
```

**Fix:**
1. Check internet connection
2. Verify firewall allows WebSocket connections
3. Try different network (VPN might block)

---

### **No audio chunks sent**
```
[AI-AGENT] âœ… TEST 2.3 PASS: Receiving audio frames
(but no TEST 3.2 messages)
```

**Fix:**
1. Check OpenAI connection succeeded (Test 3.1)
2. Verify `this.openai.connected === true`
3. Look for audio conversion errors in logs

---

### **No transcription received**
```
[OPENAI] ğŸ¤ Speech detected (VAD)
[OPENAI] ğŸ”‡ Speech ended (VAD)
(but no transcription)
```

**Fix:**
1. Speak louder and clearer
2. Speak for at least 1-2 seconds
3. Check `input_audio_transcription` is enabled
4. Wait longer (can take 2-3 seconds)

---

### **Garbage transcriptions**
```
[OPENAI] ğŸ“ Transcription: "asdfjkl;qwer"
```

**Fix:**
1. Check audio format conversion is correct
2. Verify sample rate is 24kHz
3. Test with simple phrases first
4. Check microphone quality in browser

---

## What's NOT Implemented Yet

âŒ **AI audio response playback** (Step 4)  
âŒ **Publishing AI voice to LiveKit** (Step 4)  
âŒ **User hearing AI speak** (Step 4)

Currently, the AI:
- âœ… Receives user audio
- âœ… Transcribes speech
- âœ… Generates text responses
- âŒ Does NOT play audio back yet

---

## Success Criteria

**Ready for Step 4 when:**
- âœ… Test 3.1 PASS - OpenAI connected
- âœ… Test 3.2 PASS - Audio chunks streaming
- âœ… Test 3.3 PASS - Accurate transcriptions
- âœ… AI responses are relevant and timely

**Do NOT proceed to Step 4 until all tests pass!**

---

## Commands Reference

```bash
# Start AI agent with OpenAI integration
cd backend
npm run agent

# Check logs for test results
# Look for: TEST 3.1, TEST 3.2, TEST 3.3

# Stop AI agent
Ctrl+C
```

---

## Next Step

**Step 4:** Receive AI audio responses from OpenAI and publish them back to LiveKit so the user can hear the AI speaking! ğŸ™ï¸
